{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPs30Skj9fzQd2ktfqQltqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sneha-mv23/PetVision/blob/main/dataset_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugVANLq8EIsj"
      },
      "outputs": [],
      "source": [
        "# Download Stanford Dogs image dataset\n",
        "!wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b44ZSYg2Ew8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder and extract images.tar\n",
        "!mkdir images\n",
        "!tar -xf images.tar -C images\n"
      ],
      "metadata": {
        "id": "hNJOXIr6FSKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, random\n",
        "\n",
        "def split_dataset(source_dir, dest_dir, train_ratio=0.7, val_ratio=0.15):\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(dest_dir, split), exist_ok=True)\n",
        "\n",
        "    for breed in os.listdir(source_dir):\n",
        "        breed_path = os.path.join(source_dir, breed)\n",
        "        if not os.path.isdir(breed_path):\n",
        "            continue\n",
        "        images = os.listdir(breed_path)\n",
        "        random.shuffle(images)\n",
        "\n",
        "        train_end = int(len(images) * train_ratio)\n",
        "        val_end = int(len(images) * (train_ratio + val_ratio))\n",
        "\n",
        "        for i, img in enumerate(images):\n",
        "            if i < train_end:\n",
        "                split = 'train'\n",
        "            elif i < val_end:\n",
        "                split = 'val'\n",
        "            else:\n",
        "                split = 'test'\n",
        "\n",
        "            dest_path = os.path.join(dest_dir, split, breed)\n",
        "            os.makedirs(dest_path, exist_ok=True)\n",
        "\n",
        "            shutil.copy(os.path.join(breed_path, img), os.path.join(dest_path, img))\n",
        "\n",
        "# Call the function to split the dataset\n",
        "split_dataset(\"images/Images\", \"data\")\n"
      ],
      "metadata": {
        "id": "AtokXhIzGvWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    path = os.path.join(\"data\", split)\n",
        "    total_images = sum([len(files) for r, d, files in os.walk(path)])\n",
        "    print(f\"{split} images: {total_images}\")\n"
      ],
      "metadata": {
        "id": "KK-wI-zRG_VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "image_size = 224  # Standard input size for CNNs like ResNet, VGG\n",
        "\n",
        "# For training: Resize, RandomCrop, Flip, Normalize\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                         std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# For validation and test: Resize and Normalize only\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
        "                         std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Define paths\n",
        "train_path = 'data/train'\n",
        "val_path = 'data/val'\n",
        "test_path = 'data/test'\n",
        "\n",
        "# Load datasets with transforms\n",
        "train_dataset = datasets.ImageFolder(root=train_path, transform=train_transforms)\n",
        "val_dataset = datasets.ImageFolder(root=val_path, transform=val_test_transforms)\n",
        "test_dataset = datasets.ImageFolder(root=test_path, transform=val_test_transforms)\n",
        "\n",
        "batch_size = 32  # You can try 64 or 128 later\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "tQO2lmXmEyKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "# Helper function to unnormalize and show image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get one batch from train loader\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show 1st 8 images in the batch\n",
        "imshow(torchvision.utils.make_grid(images[:8]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RPpiWBlaE3aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class PetVisionCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(PetVisionCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (B, 3, 224, 224) -> (B, 32, 224, 224)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),                         # (B, 32, 112, 112)\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),                         # (B, 64, 56, 56)\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)                          # (B, 128, 28, 28)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),                     # Flatten to (B, 128*28*28)\n",
        "            nn.Linear(128 * 28 * 28, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    # Number of classes = number of dog breeds\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "# Instantiate the model\n",
        "model = PetVisionCNN(num_classes)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "val_accuracy = 100 * correct / total\n",
        "print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "torch.save(model.state_dict(), 'petvision_model.pth')"
      ],
      "metadata": {
        "id": "tGAvpHi9FAz8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}